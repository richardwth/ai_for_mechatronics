{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from GeneralTools.misc_fun import FLAGS\n",
    "\n",
    "FLAGS.TENSORFLOW_VERSION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of b is Tensor(\"mul_3:0\", shape=(2,), dtype=float32)\nThe shape of b is [2]\nThe name/operation of b is 'mul_3:0'\nThe name/operation of b is 'b_3:0'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\"\"\"\n",
    "Session\n",
    "\n",
    "For TensorFlow 1.x versions, the default process is to declare a graph and \n",
    "call a session to run it.\n",
    "\"\"\"\n",
    "\n",
    "a = tf.constant([3.0, 2.0], dtype=tf.float32)\n",
    "b = a * 2.0\n",
    "b1 = tf.multiply(a, 2.0, name='b')\n",
    "# rerun this block, you will see the results change, why?\n",
    "print('The value of b is {}'.format(b))\n",
    "print('The shape of b is {}'.format(b.get_shape().as_list()))  # is b a row vector or column?\n",
    "print(\"The name/operation of b is '{}'\".format(b.name))\n",
    "print(\"The name/operation of b is '{}'\".format(b1.name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of b is [6. 4.]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "bv = sess.run(b)\n",
    "print('The value of b is {}'.format(bv))\n",
    "# remember to call sess.close() by the end of code to release the resources\n",
    "sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of b is [6. 4.]\nThe value of b is [6. 4.]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Interactive session saves the effort of calling sess.run many times\n",
    "Instead, we call node.eval() for any node if the graph\n",
    "\"\"\"\n",
    "sess = tf.InteractiveSession()\n",
    "print('The value of b is {}'.format(b.eval()))\n",
    "sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of b is [6. 4.]\nThe value of b is [6. 4.]\n"
     ]
    }
   ],
   "source": [
    "# To avoid calling sess.close()\n",
    "with tf.Session() as sess:\n",
    "    bv = sess.run(b)\n",
    "    print('The value of b is {}'.format(bv))\n",
    "    print('The value of b is {}'.format(b.eval()))\n",
    "\n",
    "print('whatever')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of b is [6. 4.]\nThe shape of b is (2,)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Starting from TensorFlow 2.0, the default process is eager execution.\n",
    "That is, TF evaluates operations immediately without declaring it in the graph.\n",
    "For TensorFlow 1.x versions, we can manually call eager model.\n",
    "See the below link for more info:\n",
    "https://www.tensorflow.org/guide/eager\n",
    "\n",
    "Eager execution has to be called at the program startup.\n",
    "Once called, all following operations are executed in eager mode.\n",
    "\n",
    "It is generally believed that graph mode is comparable or more efficient than eager mode.\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "a = tf.constant([3.0, 2.0], dtype=tf.float32)\n",
    "b = a * 2.0\n",
    "# rerun this block, you will see the results change, why?\n",
    "print('The value of b is {}'.format(b))\n",
    "print('The shape of b is {}'.format(b.shape))  # instead of b.get_shape().as_list()\n",
    "# print(\"The name/operation of b is '{}'\".format(b.name)) --> \n",
    "# Tensor.name is meaningless when eager execution is enabled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train has shape (60000, 28, 28), data type uint8 and range (0, 255)\ny_train has shape (60000,) and data type uint8\nThe shape of x_test is (10000, 28, 28)\nThe shape of x_test is (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\"\"\"\n",
    "Data pipeline\n",
    "\n",
    "It is highly suggested to use the tf.data API to handle large datasets\n",
    "    tf.data.Dataset \n",
    "        reads data from one or several source files\n",
    "        does pre-processing\n",
    "    tf.data.Iterator\n",
    "        defines an iterator that reads a batch of data at each run\n",
    "\n",
    "If the dataset is small, say, 200 MB, \n",
    "it is OK to load the dataset into memory (placeholder would be better).\n",
    "If the dataset is large, almost 1 GB to many GBs,\n",
    "it is highly encouraged to store the dataset in disk.\n",
    "\"\"\"\n",
    "# example 1, small dataset, save to graph\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    mnist = tf.keras.datasets.mnist\n",
    "    (x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "    # for this small dataset, we can do pre-processing here.\n",
    "    # but for illustration purpose, we handle it later\n",
    "    # x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "    print('x_train has shape {}, data type {} and range {}'.format(\n",
    "        x_train.shape, x_train.dtype, \n",
    "        (np.amin(x_train), np.amax(x_train))))\n",
    "    print('y_train has shape {} and data type {}'.format(\n",
    "        y_train.shape, y_train.dtype))\n",
    "    print('The shape of x_test is {}'.format(x_test.shape))\n",
    "    print('The shape of x_test is {}'.format(x_test.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tf.float32, tf.int32)\n(TensorShape([Dimension(28), Dimension(28)]), TensorShape([]))\n"
     ]
    }
   ],
   "source": [
    "with graph.as_default():\n",
    "    \"\"\"\n",
    "    All tensors in a dataset must have the same number of samples\n",
    "    float32 is often used in TF\n",
    "    Input to from_tensor_slices does not have to be a dictionary; \n",
    "    tensor, tuple, list are acceptable.\n",
    "    \"\"\"\n",
    "    dataset_tr = tf.data.Dataset.from_tensor_slices(\n",
    "        {\"features\": tf.constant(x_train, dtype=tf.float32),\n",
    "         \"labels\": tf.constant(y_train, dtype=tf.int32)})\n",
    "    # apply parser or pre-processing\n",
    "    # if we have not scaled the data before, we can scale it here\n",
    "    dataset_tr = dataset_tr.map(\n",
    "        lambda d: (d['features'] / 255.0, d['labels']))\n",
    "    print(dataset_tr.output_types)\n",
    "    print(dataset_tr.output_shapes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"IteratorGetNext:0\", shape=(?, 28, 28), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Schedule an iteration procedure\n",
    "\n",
    "When the number of samples in datasets cannot be divided by batch size,\n",
    "we may consider skip some samples...\n",
    "\"\"\"\n",
    "skip_count = 0\n",
    "shuffle_data = True\n",
    "batch_size = 400\n",
    "buffer_size = 10000\n",
    "num_epoch = 1  # set this to None or -1 will repeat the dataset infinite times\n",
    "with graph.as_default():\n",
    "    if skip_count > 0:\n",
    "        print('Number of {} instances skipped.'.format(skip_count))\n",
    "        dataset_tr = dataset_tr.skip(skip_count)\n",
    "    # shuffle\n",
    "    if shuffle_data:\n",
    "        dataset_tr = dataset_tr.shuffle(buffer_size)\n",
    "    # make batch\n",
    "    dataset_tr = dataset_tr.batch(batch_size)\n",
    "    # repeat datasets for num_epoch\n",
    "    dataset_tr = dataset_tr.repeat(num_epoch)\n",
    "    \n",
    "    \"\"\"\n",
    "    Define an iterator that reads an element (here, a batch) each time\n",
    "    \n",
    "    Several iterators are available, see this link below for more details:\n",
    "    https://www.tensorflow.org/guide/datasets\n",
    "    \"\"\"\n",
    "    iterator = dataset_tr.make_one_shot_iterator()\n",
    "    # read a batch\n",
    "    x_batch, y_batch = iterator.get_next()\n",
    "    # This is the end of input pipeline\n",
    "    print(x_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_batch is (400, 28, 28)\nShape of y_batch is (400,)\n"
     ]
    }
   ],
   "source": [
    "# call a session to actually read the data\n",
    "with graph.as_default():\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        xv, yv = sess.run([x_batch, y_batch])\n",
    "        \n",
    "        print('Shape of x_batch is {}'.format(xv.shape))\n",
    "        print('Shape of y_batch is {}'.format(yv.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\richa\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:31: UserWarning: Label file C:/Users/richa/PycharmProjects/ai_mechatronics/Results/mnist\\embedding_image_raw\\mnist_label.tsv already exists, thus this step is ignored.\nC:\\Users\\richa\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:42: UserWarning: Sprite file C:/Users/richa/PycharmProjects/ai_mechatronics/Results/mnist\\embedding_image_raw\\mnist.png already exists, thus this step is ignored.\n"
     ]
    }
   ],
   "source": [
    "import os.path, os, warnings, imageio\n",
    "from GeneralTools.misc_fun import FLAGS\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "import numpy as np\n",
    "\"\"\"\n",
    "Let's visualize the data!\n",
    "\n",
    "A tutorial can be found at:\n",
    "https://www.easy-tensorflow.com/tf-tutorials/tensorboard/tb-embedding-visualization\n",
    "\"\"\"\n",
    "# prepare folder\n",
    "folder = 'mnist'\n",
    "subfolder = 'embedding_image_raw'\n",
    "filename = 'mnist'\n",
    "summary_folder = os.path.join(FLAGS.DEFAULT_OUT, folder, subfolder)\n",
    "if not os.path.exists(summary_folder):\n",
    "    os.makedirs(summary_folder)\n",
    "embedding_path = os.path.join(summary_folder, filename + '_embedding.ckpt')\n",
    "sprite_path = os.path.join(summary_folder, filename + '.png')\n",
    "label_path = os.path.join(summary_folder, filename + '_label.tsv')\n",
    "\n",
    "# prepare data, sprite images and files\n",
    "embedding_data = np.reshape(xv, (batch_size, -1))\n",
    "images = xv  # shape [400, 28, 28]\n",
    "image_size = xv.shape[1:]  # [28, 28]\n",
    "labels = yv\n",
    "\n",
    "# write label to file\n",
    "if os.path.isfile(label_path):\n",
    "    warnings.warn(\n",
    "        'Label file {} already exists, thus this step is ignored.'.format(label_path))\n",
    "else:\n",
    "    metadata_file = open(label_path, 'w')\n",
    "    metadata_file.write('Name\\tClass\\n')\n",
    "    for index, label in enumerate(labels):\n",
    "            metadata_file.write('%06d\\t%s\\n' % (index, str(label)))\n",
    "    metadata_file.close()\n",
    "\n",
    "# write images to sprite\n",
    "if os.path.isfile(sprite_path):\n",
    "    warnings.warn(\n",
    "        'Sprite file {} already exists, thus this step is ignored.'.format(sprite_path))\n",
    "else:\n",
    "    # extend image shapes to [batch size, height, width, 3]\n",
    "    if len(images.shape) == 3:  # if dimension of image is 3, extend it to 4\n",
    "        images = np.tile(images[..., np.newaxis], (1, 1, 1, 3))\n",
    "        print('Shape of images has been changed to {}'.format(images.shape))\n",
    "    if images.shape[3] == 1:  # if last dimension is 1, extend it to 3\n",
    "        images = np.tile(images, (1, 1, 1, 3))\n",
    "        print('Shape of images has been changed to {}'.format(images.shape))\n",
    "        \n",
    "    # scale image to range [0,1]\n",
    "    # we have done this step in pre-processing so no worries\n",
    "    \n",
    "    # invert images for mnist\n",
    "    images = 1 - images\n",
    "    \n",
    "    # Tile the individual thumbnails into an image\n",
    "    mesh_num = (20, 20)\n",
    "    new_shape = mesh_num + images.shape[1:]  # (20, 20, 28, 28, 3)\n",
    "    images = images.reshape(new_shape).transpose((0, 2, 1, 3, 4))\n",
    "    print('Shape of images has been changed to {}'.format(images.shape))\n",
    "    images = images.reshape(\n",
    "        (mesh_num[0] * images.shape[1], mesh_num[1] * images.shape[3]) + images.shape[4:])\n",
    "    print('Shape of images has been changed to {}'.format(images.shape))\n",
    "    images = (images * 255).astype(np.uint8)\n",
    "    # save images to file\n",
    "    imageio.imwrite(sprite_path, images)\n",
    "\n",
    "# write data to ckpt\n",
    "if os.path.isfile(embedding_path):\n",
    "    warnings.warn(\n",
    "        'Embedding file {} already exists, thus this step is ignored.'.format(embedding_path))\n",
    "else:\n",
    "    # register a session\n",
    "    sess = tf.Session()\n",
    "    # prepare a embedding variable\n",
    "    # note this must be a variable, not a tensor/constant\n",
    "    embedding_var = tf.Variable(embedding_data, name='em_data')\n",
    "    sess.run(embedding_var.initializer)\n",
    "    # configure the embedding projector\n",
    "    config = projector.ProjectorConfig()\n",
    "    embedding = config.embeddings.add()\n",
    "    embedding.tensor_name = embedding_var.name\n",
    "    # add metadata (label) to embedding\n",
    "    if label_path is not None:\n",
    "        embedding.metadata_path = label_path\n",
    "    # add sprite image to embedding\n",
    "    if sprite_path is not None:\n",
    "        embedding.sprite.image_path = sprite_path\n",
    "        embedding.sprite.single_image_dim.extend(image_size)\n",
    "    # finalize embedding setting\n",
    "    embedding_writer = tf.summary.FileWriter(summary_folder)\n",
    "    projector.visualize_embeddings(embedding_writer, config)\n",
    "    embedding_saver = tf.train.Saver([embedding_var], max_to_keep=1)\n",
    "    embedding_saver.save(sess, embedding_path)\n",
    "    # close all\n",
    "    sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\richa\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:31: UserWarning: Label file C:/Users/richa/PycharmProjects/ai_mechatronics/Results/mnist\\embedding_image_raw\\mnist_label.tsv already exists, thus this step is ignored.\nC:\\Users\\richa\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:42: UserWarning: Sprite file C:/Users/richa/PycharmProjects/ai_mechatronics/Results/mnist\\embedding_image_raw\\mnist.png already exists, thus this step is ignored.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\"\"\"\n",
    "Data pipeline\n",
    "\n",
    "It is highly suggested to use the tf.data API to handle large datasets\n",
    "    tf.data.Dataset \n",
    "        reads data from one or several source files\n",
    "        does pre-processing\n",
    "    tf.data.Iterator\n",
    "        defines an iterator that reads a batch of data at each run\n",
    "\n",
    "If the dataset is small, say, 200 MB, \n",
    "it is OK to load the dataset into memory (placeholder would be better).\n",
    "If the dataset is large, almost 1 GB to many GBs,\n",
    "it is highly encouraged to store the dataset in disk.\n",
    "\"\"\"\n",
    "# example 2, small dataset, placeholder\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    mnist = tf.keras.datasets.mnist\n",
    "    (x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "    # for this small dataset, we can do pre-processing here.\n",
    "    # but for illustration purpose, we handle it later\n",
    "    # x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "    print('x_train has shape {}, data type {} and range {}'.format(\n",
    "        x_train.shape, x_train.dtype, \n",
    "        (np.amin(x_train), np.amax(x_train))))\n",
    "    print('y_train has shape {} and data type {}'.format(\n",
    "        y_train.shape, y_train.dtype))\n",
    "    print('The shape of x_test is {}'.format(x_test.shape))\n",
    "    print('The shape of x_test is {}'.format(x_test.shape))\n",
    "    \n",
    "    \"\"\"\n",
    "    All tensors in a dataset must have the same number of samples\n",
    "    float32 is often used in TF\n",
    "    Input to from_tensor_slices does not have to be a dictionary; \n",
    "    tensor, tuple, list are acceptable.\n",
    "    \"\"\"\n",
    "    features_placeholder = tf.placeholder(tf.float32, x_train.shape)\n",
    "    labels_placeholder = tf.placeholder(tf.int32, y_train.shape)\n",
    "    dataset_tr = tf.data.Dataset.from_tensor_slices(\n",
    "        {\"features\": features_placeholder,\n",
    "         \"labels\": labels_placeholder})\n",
    "    # apply parser or pre-processing\n",
    "    # if we have not scaled the data before, we can scale it here\n",
    "    dataset_tr = dataset_tr.map(\n",
    "        lambda d: (d['features'] / 255.0, d['labels']))\n",
    "    print(dataset_tr.output_types)\n",
    "    print(dataset_tr.output_shapes)\n",
    "    \n",
    "    # pre-process the data\n",
    "    skip_count = 0\n",
    "    shuffle_data = True\n",
    "    batch_size = 400\n",
    "    buffer_size = 10000\n",
    "    num_epoch = 1  # set this to None or -1 will repeat the dataset infinite times\n",
    "    if skip_count > 0:\n",
    "        print('Number of {} instances skipped.'.format(skip_count))\n",
    "        dataset_tr = dataset_tr.skip(skip_count)\n",
    "    # shuffle\n",
    "    if shuffle_data:\n",
    "        dataset_tr = dataset_tr.shuffle(buffer_size)\n",
    "    # make batch\n",
    "    dataset_tr = dataset_tr.batch(batch_size)\n",
    "    # repeat datasets for num_epoch\n",
    "    dataset_tr = dataset_tr.repeat(num_epoch)\n",
    "    \n",
    "    \"\"\"\n",
    "    Define an iterator that reads an element (here, a batch) each time\n",
    "    \n",
    "    Several iterators are available, see this link below for more details:\n",
    "    https://www.tensorflow.org/guide/datasets\n",
    "    \"\"\"\n",
    "    iterator = dataset_tr.make_initializable_iterator()\n",
    "    # read a batch\n",
    "    x_batch, y_batch = iterator.get_next()\n",
    "    # This is the end of input pipeline\n",
    "    print(x_batch)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        # must run initializer for iterator first that feeds in dataset\n",
    "        sess.run(\n",
    "            iterator.initializer, \n",
    "            feed_dict={features_placeholder: x_train, labels_placeholder: y_train})\n",
    "        xv, yv = sess.run([x_batch, y_batch])\n",
    "        \n",
    "        print('Shape of x_batch is {}'.format(xv.shape))\n",
    "        print('Shape of y_batch is {}'.format(yv.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
