{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train has shape (60000, 784), data type float64 and range (0.0, 1.0)\ny_train has shape (60000,) and data type uint8\nThe shape of x_test is (10000, 784)\nThe shape of y_test is (10000,)\nAfter one-hot coding, y_train has shape (60000, 10) and data type int32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nWARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\nFor more information, please see:\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n  * https://github.com/tensorflow/addons\nIf you depend on functionality not listed there, please file an issue.\n\nTraining set output types (tf.float64, tf.int32)\nTraining set output shapes (TensorShape([Dimension(None), Dimension(784)]), TensorShape([Dimension(None), Dimension(10)]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set output types (tf.float64, tf.int32)\nValidation set output shapes (TensorShape([Dimension(None), Dimension(784)]), TensorShape([Dimension(None), Dimension(10)]))\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\"\"\"\n",
    "This jupyter notebook provides an example of using Keras to complete\n",
    "some of the tasks in Project 1.\n",
    "\"\"\"\n",
    "\n",
    "print(tf.keras.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train has shape (60000, 784), data type float64 and range (0.0, 1.0)\ny_train has shape (60000,) and data type uint8\nThe shape of x_test is (10000, 784)\nThe shape of y_test is (10000,)\nAfter one-hot coding, y_train has shape (60000, 10) and data type int32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nWARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\nFor more information, please see:\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n  * https://github.com/tensorflow/addons\nIf you depend on functionality not listed there, please file an issue.\n\nTraining set output types (tf.float64, tf.int32)\nTraining set output shapes (TensorShape([Dimension(None), Dimension(784)]), TensorShape([Dimension(None), Dimension(10)]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set output types (tf.float64, tf.int32)\nValidation set output shapes (TensorShape([Dimension(None), Dimension(784)]), TensorShape([Dimension(None), Dimension(10)]))\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\"\"\"\n",
    "Prepare the dataset\n",
    "\n",
    "Here I intentionally used tf.data.Dataset.from_tensor_slices \n",
    "to handle the input data, for two reasons:\n",
    "1. To show that tf.keras can blend well with TensorFlow functions\n",
    "2. For large dataset, tf.keras may benefit from tf.data.TFRecordDataset.\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 100\n",
    "batch_size_va = 600\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "    # for this small dataset, we can do pre-processing here.\n",
    "    x_train = np.reshape(x_train / 255.0, (x_train.shape[0], -1))\n",
    "    x_test = np.reshape(x_test / 255.0, (x_test.shape[0], -1))\n",
    "    print('x_train has shape {}, data type {} and range {}'.format(\n",
    "        x_train.shape, x_train.dtype, \n",
    "        (np.amin(x_train), np.amax(x_train))))\n",
    "    print('y_train has shape {} and data type {}'.format(\n",
    "        y_train.shape, y_train.dtype))\n",
    "    print('The shape of x_test is {}'.format(x_test.shape))\n",
    "    print('The shape of y_test is {}'.format(y_test.shape))\n",
    "    \n",
    "    # convert label to one-hot embedding as requested by Keras\n",
    "    y_train = tf.keras.utils.to_categorical(y_train, num_classes=10, dtype='int32')\n",
    "    y_test = tf.keras.utils.to_categorical(y_test, num_classes=10, dtype='int32')\n",
    "    print('After one-hot coding, y_train has shape {} and data type {}'.format(\n",
    "        y_train.shape, y_train.dtype))\n",
    "    \n",
    "    # training set\n",
    "    with tf.name_scope('data_tr'):\n",
    "        dataset_tr = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "        # apply parser or pre-processing\n",
    "        # if we have not scaled the data before, we can scale it here\n",
    "        # dataset_tr = dataset_tr.map(lambda t: (t[0] / 255.0, t[1]))\n",
    "        dataset_tr = dataset_tr.shuffle(1000).batch(batch_size).repeat()\n",
    "        dataset_tr = dataset_tr.prefetch(tf.contrib.data.AUTOTUNE)\n",
    "    print('Training set output types {}'.format(dataset_tr.output_types))\n",
    "    print('Training set output shapes {}'.format(dataset_tr.output_shapes))\n",
    "    \n",
    "    # validation set\n",
    "    with tf.name_scope('data_va'):\n",
    "        dataset_va = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "        # apply parser or pre-processing\n",
    "        # if we have not scaled the data before, we can scale it here\n",
    "        # dataset_tr = dataset_tr.map(lambda t: (t[0] / 255.0, t[1]))\n",
    "        dataset_va = dataset_va.batch(batch_size_va).repeat()\n",
    "        dataset_va = dataset_va.prefetch(tf.contrib.data.AUTOTUNE)\n",
    "    print('Validation set output types {}'.format(dataset_va.output_types))\n",
    "    print('Validation set output shapes {}'.format(dataset_va.output_shapes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\richa\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\richa\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import timeline\n",
    "\"\"\"\n",
    "Define and train the model\n",
    "\"\"\"\n",
    "num_epoch = 1\n",
    "summary_folder = 'C:/Users/richa/PycharmProjects/ai_mechatronics/Results/mnist/keras_log/'\n",
    "do_trace = False\n",
    "\n",
    "with graph.as_default():\n",
    "    # define the model\n",
    "    mdl = tf.keras.Sequential(name='MLP')\n",
    "    # Adds a MLP\n",
    "    mdl.add(\n",
    "        tf.keras.layers.Dense(\n",
    "            384, activation='relu', name='l1', \n",
    "            batch_input_shape=(None, 784)))\n",
    "    mdl.add(tf.keras.layers.Dense(128, activation='relu', name='l2'))\n",
    "    mdl.add(tf.keras.layers.Dense(10, activation='softmax', name='out'))\n",
    "    \"\"\"\n",
    "    Alternatively, you may define the models as below, which is very much like \n",
    "    TensorFlow logic. \n",
    "    \n",
    "    from tf.keras.layers import Input, Conv2D, MaxPool2D, Flatten, Dense, Dropout\n",
    "    \n",
    "    # assume train and test data has been reshaped to (None, 28, 28, 1)\n",
    "    inputs = Input(shape=(28, 28, 1))  # Input(shape=(784, )) for vector input\n",
    "    hidden = Conv2D(32, (3, 3),activation='relu', padding='valid')(inputs)\n",
    "    hidden = MaxPool2D(pool_size=(2, 2))(hidden)\n",
    "    hidden = Conv2D(64, (3, 3), activation='relu')(hidden)\n",
    "    hidden = MaxPool2D(pool_size=(2, 2))(hidden)\n",
    "    hidden = Flatten()(hidden)\n",
    "    hidden = Dense(512, activation='relu')(hidden)\n",
    "    hidden = Dropout(0.5)(hidden)\n",
    "    outputs = Dense(10, activation='softmax')(hidden)\n",
    "\n",
    "    mdl = tf.keras.Model(inputs, outputs)\n",
    "    \"\"\"\n",
    "    \n",
    "    # configure profiling\n",
    "    if do_trace:\n",
    "        run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "        run_metadata = tf.RunMetadata()\n",
    "    else:\n",
    "        run_options = None\n",
    "        run_metadata = None\n",
    "    \n",
    "    # configure training process\n",
    "    mdl.compile(optimizer=tf.train.AdamOptimizer(0.01),\n",
    "                loss='categorical_crossentropy', metrics=['accuracy'], \n",
    "                options=run_options, run_metadata=run_metadata)\n",
    "    \n",
    "    \"\"\"\n",
    "    Configure summary\n",
    "    when write_graph=True, the event file size become quite large, \n",
    "    almost 1 GB in my case.\n",
    "    \"\"\"\n",
    "    callbacks = [tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=summary_folder, histogram_freq=1, batch_size=batch_size, \n",
    "        write_graph=False, write_grads=True)]\n",
    "    \n",
    "    # fit the model\n",
    "    if do_trace:\n",
    "        \"\"\"\n",
    "        The reason I separate the tracing part from mdl.fit is that I observed\n",
    "        sometimes mdl.fit yields an empty timeline.json file (size 1kb).\n",
    "        \n",
    "        Unfortunately, I have not figured out how to do tracing for multiple runs\n",
    "        using purely keras.\n",
    "        \"\"\"\n",
    "        for steps in range(2):\n",
    "            mdl.train_on_batch(dataset_tr)\n",
    "        # get the runtime statistics\n",
    "        trace = timeline.Timeline(step_stats=run_metadata.step_stats)\n",
    "        chrome_trace = trace.generate_chrome_trace_format()\n",
    "        with open(summary_folder + 'timeline.json', 'w') as f:\n",
    "            f.write(chrome_trace)\n",
    "    else:\n",
    "        steps_per_epoch = x_train.shape[0] // batch_size\n",
    "        print('There are {} steps in one training epoch'.format(steps_per_epoch))\n",
    "        mdl.fit(\n",
    "            dataset_tr, epochs=num_epoch, steps_per_epoch=steps_per_epoch,\n",
    "            validation_data=dataset_va, validation_steps=1, callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\"\"\"\n",
    "Save the model\n",
    "\"\"\"\n",
    "ckpt_folder = 'C:/Users/richa/PycharmProjects/ai_mechatronics/Results/mnist/keras_ckpt/'\n",
    "ckpt_name = 'mnist_ckpt'\n",
    "with graph.as_default():\n",
    "    mdl.save_weights(ckpt_folder + ckpt_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
