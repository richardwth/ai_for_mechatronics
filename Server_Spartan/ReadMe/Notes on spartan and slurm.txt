Login:
ssh richard_wth@spartan.hpc.unimelb.edu.au
pw: uiquiorra950288

Spartan commands:
sbash xxx.slurm: submit a job
sinteractive: interactive mode
sinteractive --partition=shortgpgpu --gres=gpu:1: interactive mode with GPU
squeue -j jobid: check status of jobid
squeue -u richard_wth: check all jobs of a user
scancel jobid: cancel a job

Change file owner and group after cp/mv: chown
e.g. change all files recursiverly (-R) in folder celebA/ to owner richard_wth and group punim0512
chown -R richard_wth:punim0512 /data/cephfs/punim0512/celebA/

Slurm example:

CPU example:
#!/bin/bash
#SBATCH --job-name=test
#SBATCH -p cloud
#SBATCH --time=00:01:00
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=8
#SBATCH --mail-user=weiw8@student.unimelb.edu.au
#SBATCH --mail-type=ALL
module load Python/3.6.1-intel-2017.u2
python test.py

GPU example 1
#!/bin/bash
#SBATCH --job-name=test
#SBATCH --partition gpu 
#SBATCH --gres=gpu
#SBATCH --time=00:01:00
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=8
module load cuDNN/6.0-intel-2017.u2-CUDA-8.0.44
module load Python/3.5.2-intel-2017.u2
module load Tensorflow/1.4.0-intel-2017.u2-Python-3.5.2-gpu
python test.py

GPU example 2
#!/bin/bash
#SBATCH --job-name=logistic
#SBATCH --partition=gpu 
#SBATCH --gres=gpu:1
#SBATCH --time=20:00:00
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=8
#SBATCH --mail-user=weiw8@student.unimelb.edu.au
#SBATCH --mail-type=ALL
module load Python/3.5.2-GCC-6.2.0-CUDA9
module load Tensorflow/1.7.0-GCC-6.2.0-Python-3.5.2-GPU
module load CUDA/9.0.176-GCC-6.2.0
module load cuDNN/7.0.3-intel-2017.u2-CUDA-9.0.176
python cifar_sngan.py > cifar_sngan_ttur_2e-4_5e-4.txt

For more than 1 gpu: #SBATCH --gres=gpu:2
For p100 gpu: #SBATCH --partition=gpgpu
For v100 gpu: #SBATCH --partition=deeplearn
